{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2883e64",
   "metadata": {},
   "source": [
    "# $$Weights$$\n",
    "\n",
    "$$y = Wx$$\n",
    "\n",
    "$$W = \\begin{bmatrix}\n",
    "\\cdots & W_{1}^T & \\cdots \\\\\n",
    "\\cdots & W_{2}^T & \\cdots \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\cdots & W_{n}^T & \\cdots \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "- There is one row in **W** for each unit in the layer. The same as the number of outputs of this layer.\n",
    "- The number of elements in **w** is the number of **input(x)**. This is the number of inputs to this layer.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/Sayan-Roy-729/images/blob/main/deep_learning/11_weights/images/image-1.png?raw=true\" style=\"border-radius: 10px\"/>\n",
    "\n",
    "**Code:**\n",
    "- [Part 1 - Explanation of Weight Matrix Sizes](https://github.com/Sayan-Roy-729/Data-Science/blob/main/Deep%20Learning/Using%20Pytorch/Part%2011%20-%20Weights/Part%201%20-%20Explanation%20of%20Weight%20Matrix%20Sizes.ipynb)\n",
    "\n",
    "## A Surprising Demo of Weight Initializations\n",
    "\n",
    "**Take-Home Message:**\n",
    "- Models cannot learn when all trainable parameters are initialized to the same value.\n",
    "- Models can learn as long as some trainable parameters are initialized to different numbers.\n",
    "- Why is this??!!??!!\n",
    "\n",
    "**Code:**\n",
    "- [Part 2 - A Surprising Demonstration of Weight Initializations](https://github.com/Sayan-Roy-729/Data-Science/blob/main/Deep%20Learning/Using%20Pytorch/Part%2011%20-%20Weights/Part%202%20-%20A%20Surprising%20Demonstration%20of%20Weight%20Initializations.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76a1174",
   "metadata": {},
   "source": [
    "## Theory: Why and how to initialize weights\n",
    ">**Weight Initializations:**\n",
    "- Complex, multidimensional landscapes have few local minima.\n",
    "- Initializing the weights to be random numbers means that we are exceedingly unlikely to start in or near a local minima (for DL).\n",
    "- Random initialization provides the necessary rough texture for g.d. to move.\n",
    "- Analogy: Walk downhill in Bonneville salt flats vs Badlands park.\n",
    "- Random weights allow for any direction, even if it's (initially) the wrong one.\n",
    "- From engineering: \"Stochastic facilitation\"\n",
    "\n",
    ">**Weight initialization: the algebrac view**\n",
    "- If all weights are the same, there is no diversity.\n",
    "- With random weights, some will be strengthened, some weakened, some by a lot and some by a little.\n",
    "- Equal weights is called \"weight symmetry\" and thus randomizing weights is \"breaking symmetry\".\n",
    "\n",
    ">**So, how to initialize the weights?**\n",
    "- Random numbers drawn from a normal (Gaussian) distribution.\n",
    "- With random weights, some will be strengthened, some weakened, some by a lot and some by a little.\n",
    "- Standard deviation should be relatively small.\n",
    "- Small weights (close to zero) increase risk of vanishing gradients.\n",
    "- Large weights increase risk of exploding gradients.\n",
    "- Solution: Set the variance of the weights proportional to the size of the network.\n",
    "- Initializing biases is less important that weights.\n",
    "\n",
    "![Methods](https://github.com/Sayan-Roy-729/images/blob/main/deep_learning/11_weights/images/image-2.png?raw=true)\n",
    "\n",
    "![Methods](https://github.com/Sayan-Roy-729/images/blob/main/deep_learning/11_weights/images/image-3.png?raw=true)\n",
    "\n",
    ">**Do weights initializations matter?**\n",
    "- For relatively simple models that are easy to train: No, not really. Just break symmetry and the model will be fine.\n",
    "- For very large, complex models with billions (or more!) parameters, weight initialization can be important.\n",
    "- Optimal weight initialization strategy is an active area of research in DL.\n",
    "\n",
    ">**Code:**\n",
    "- [Part 3 - CodeChallenge Weight Variance Inits](https://github.com/Sayan-Roy-729/Data-Science/blob/main/Deep%20Learning/Using%20Pytorch/Part%2011%20-%20Weights/Part%203%20-%20CodeChallenge%20Weight%20Variance%20Inits.ipynb)\n",
    "- [Part 4 - Xavier and Kaiming Initializations](https://github.com/Sayan-Roy-729/Data-Science/blob/main/Deep%20Learning/Using%20Pytorch/Part%2011%20-%20Weights/Part%204%20-%20Xavier%20and%20Kaiming%20Initializations.ipynb)\n",
    "- [Part 5 - CodeChallenge CodeChallenge: Xavier vs. Kaiming](https://github.com/Sayan-Roy-729/Data-Science/blob/main/Deep%20Learning/Using%20Pytorch/Part%2011%20-%20Weights/Part%205%20-%20CodeChallenge%20CodeChallenge%20Xavier%20vs.%20Kaiming.ipynb)\n",
    "- [Part 6 - CodeChallenge Identically random weights](https://github.com/Sayan-Roy-729/Data-Science/blob/main/Deep%20Learning/Using%20Pytorch/Part%2011%20-%20Weights/Part%206%20-%20CodeChallenge%20Identically%20random%20weights.ipynb)\n",
    "\n",
    "## Freezing Weights During Learning:\n",
    "\n",
    "**Freezing Weights:**\n",
    "- Freezing a layer means to switch off gradient descent in that layer.\n",
    "- This means that the weights will not change, and thus the layer will no longer learn.\n",
    "- Think of the parameter `requires_grad` as a toggle that switches on (`True`) of off (`False`) learning in for that weight matrix (or bias).\n",
    "\n",
    "**Why would you want to switch off learning?!!**\n",
    "- The main application is when working with downloaded (pretrained) networks.\n",
    "- The network is already trained, but you need to fine-tune it for your specific dataset.\n",
    "- This is called `transfer learning`.\n",
    "\n",
    "**Code:**\n",
    "- [Part 7 - Freezing Weights During Learning](https://github.com/Sayan-Roy-729/Data-Science/blob/main/Deep%20Learning/Using%20Pytorch/Part%2011%20-%20Weights/Part%207%20-%20Freezing%20Weights%20During%20Learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57696745",
   "metadata": {},
   "source": [
    "## Learning-Telated Changes in Weights\n",
    "\n",
    ">**Changes in weights matrices over time**\n",
    "- Obviously, the weights change over time (that's the whole point!)\n",
    "- How to quantify those changes?\n",
    "- Shape and width of distribution (histograms)\n",
    "- General changes over all weights per layer\n",
    "\n",
    ">**Metric 1: Euclidean distance to previous**\n",
    "\n",
    "$$d = \\sqrt{\\sum_{i=1}^{M}\\sum_{j=1}^{N}(W_{ij}^{(t)} - W_{ij}^{(t-1)^2})}$$\n",
    "\n",
    "- **Explanation in words:** Subtract two matrices, square each element, sum the entire matrix, take the square root.\n",
    "- **What it means:** Larger distances mean the weights are changing rapidly (a lot of learning). Small distances (close to zero) mean the weights change very little (little learning).\n",
    "\n",
    ">**Metric 2: Condition number**\n",
    "\n",
    "$$\\kappa = \\frac{\\sigma_{max}}{\\sigma_{min}}$$\n",
    "- **Explanation in words:** Compute the SVD, take the ratio of largest to smallest singular values.\n",
    "- **What it means:** Larger condition numbers indicate sparser matrices, meaning some directions are spacious while others are thin. It means the network learned specific features. Large condition numbers indicate sparse representations, or possible overfitting.\n",
    "\n",
    "![Condition number of a matrix](https://github.com/Sayan-Roy-729/images/blob/main/deep_learning/11_weights/images/image-4.png?raw=true)\n",
    "\n",
    ">**Code:**\n",
    "- [Part 8 - Weight Characteristics During Learning](https://github.com/Sayan-Roy-729/Data-Science/blob/main/Deep%20Learning/Using%20Pytorch/Part%2011%20-%20Weights/Part%208%20-%20Weight%20Characteristics%20During%20Learning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff9c35-19c1-4a84-ab42-7b71d6be2d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
