{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fad19f",
   "metadata": {},
   "source": [
    "# Convolution\n",
    "\n",
    ">**Small tangent about language:**\n",
    "- **Convoluted:** Excessively complicated and difficult to understand, like a bad sci-fi movie plot.\n",
    "- **Convolved:** Past tense of to convolve, which is a mathematical operation used in signal and image processing.\n",
    "\n",
    "![Convolution in 1D](./images/image-1.png)\n",
    "\n",
    "![Convolution in 2D](./images/image-2.png) ![Convolution in 2D](./images/image-3.png)\n",
    "\n",
    "![Convolution in 3D](./images/image-4.png)\n",
    "\n",
    "![With Bias Term](./images/image-5.png)\n",
    "\n",
    "![With Bias Term](./images/image-6.png)\n",
    "\n",
    ">**Additional Notes About Convolution**\n",
    "- Edges always cause difficulties. We deal with this through *padding*.\n",
    "- Image convolution in DL also involves downsampling. This is done via *stride* and *pooling*.\n",
    "- N kernels produces an N-layer result. These layers are called \"channels\" but they are features not RGB.\n",
    "- Use odd kernel sizes (3, 5, 7 etc.) to have an exact center.\n",
    "- Formally, CNNs implement cross-correlation, not convolution. But it doesn't actually matter because the kernels are empirically learned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6148b47",
   "metadata": {},
   "source": [
    "## Feature Maps & Convolution Kernels\n",
    "\n",
    "![Feature Maps](./images/image-7.png)\n",
    "\n",
    ">**Kernels concepts:**\n",
    "- Kernels are filters that extract features from an image. The same kernel applied to different images will give different feature maps.\n",
    "- Kernels are generally small (3x3, 5x5, 7x7).\n",
    "- In DL, kernels begin random and are learned through gradient descent. After learning, kernels are the same for all images. Using pre-trained kernels is called \"transfer learning\".\n",
    "- Kernels are not used to classify or make decisions; they are used to extract features. Those features are used for classification.\n",
    "\n",
    "![Channels Dimensions](./images/image-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacf4717",
   "metadata": {},
   "source": [
    ">**Code:**\n",
    "- [Part 1 - Convolution in code](https://github.com/Sayan-Roy-729/Data-Science/blob/main/Deep%20Learning/Using%20Pytorch/Part%2014%20-%20Convolution/Part%201%20-%20Convolution%20in%20code.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccda650b",
   "metadata": {},
   "source": [
    "## Convolution Parameters (stride, padding)\n",
    "\n",
    "![Convolution Padding](./images/image-9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3057590",
   "metadata": {},
   "source": [
    ">**Convolution & Padding:**\n",
    "- Padding is used to increase the size of the result of convolution, and match with the previous layer (or image).\n",
    "- Padding involves inserting 1+ rows and columns.\n",
    "- Added rows/columns are symmetric!\n",
    "- Padded numbers are usually zeros. It's also possible to wrap the image from top-to-bottom (circular convolution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617fcc02",
   "metadata": {},
   "source": [
    "![Stride](./images/image-10.png)\n",
    "\n",
    ">**Convolution and Stride**\n",
    "- Stride is used to decrease the size of the result of convolution. It is a mechanism of downsampling, and reduces the number of parameters in a CNN.\n",
    "- The stride parameter (should have been called skip IMHO) is an integer. Stride=1 gives the full result.\n",
    "- Stride is usually the same for rows and columns, but can be different when warranted.\n",
    "\n",
    "![Padding and Stride Formula](./images/image-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f951596",
   "metadata": {},
   "source": [
    ">**Code:**\n",
    "- [Part 2 - The Conv2 Class in PyTorch](https://github.com/Sayan-Roy-729/Data-Science/blob/main/Deep%20Learning/Using%20Pytorch/Part%2014%20-%20Convolution/Part%202%20-%20The%20Conv2%20Class%20in%20PyTorch.ipynb)\n",
    "- [Part 3 - CodeChallenge Choose the Parameters](https://github.com/Sayan-Roy-729/Data-Science/blob/main/Deep%20Learning/Using%20Pytorch/Part%2014%20-%20Convolution/Part%203%20-%20CodeChallenge%20Choose%20the%20Parameters.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f894e135",
   "metadata": {},
   "source": [
    "## Transpose Convolution\n",
    "\n",
    "![Transpose Convolution](./images/image-12.png)\n",
    "![Transpose Convolution](./images/image-13.png)\n",
    "\n",
    ">**What transpose convolution is:**\n",
    "- Transpose convolution means to scalar-multipy a kernel by each pixel in an image.\n",
    "- As long as the kernel is >1 pixel, the result will be higher resolution than the original image.\n",
    "- Transpose convolution is used for autoencoders and super-resolution CNNs.\n",
    "- Transpose convolution takes the same parameters as \"forward\" convolution: kernel size, padding, stride.\n",
    "\n",
    "$$N_h = s_h(M_h - 1) + k - 2p$$\n",
    "\n",
    "where,\n",
    "- $N_h$ = Number of pixels in output image.\n",
    "- $H_h$ = Number of pixels in input image.\n",
    "- $p$ = Padding\n",
    "- $k$ = Number of pixels in kernel (height)\n",
    "- $s_h$ = Stride\n",
    "\n",
    ">**Code:**\n",
    "- [Part 4 - Transpose convolution](https://github.com/Sayan-Roy-729/Data-Science/blob/main/Deep%20Learning/Using%20Pytorch/Part%2014%20-%20Convolution/Part%204%20-%20Transpose%20convolution.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbe2884",
   "metadata": {},
   "source": [
    "## Max/Mean Pooling\n",
    "\n",
    "![Mean Pooling](./images/image-14.png)\n",
    "\n",
    "![Max Pooling](./images/image-15.png)\n",
    "\n",
    ">**Why use a pooling layer?**\n",
    "- Reduces dimensionality (fewer parameters)\n",
    "- Selects for features over a broader spatial area (increased \"receptive field\" size)\n",
    "- Deeper into the model, we want more channels with fewer pixels. This makes the representations increasingly abstract.\n",
    "\n",
    ">**Max or Mean Pooling?**\n",
    "- *Max Pooling*: Highlishts sharp features. Useful for sparse data and increasing contrast.\n",
    "- *Mean Pooling*: Smooths images (it's a low-pass filter). Useful for noisy data and to reduce the impact of outliers on learning.\n",
    "\n",
    "![What are receptive fields?](./images/image-16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d55430",
   "metadata": {},
   "source": [
    ">**What about deep ANNs?**\n",
    "- Sufficiently deep ANNs will also work \"just as well\" (because of the universal approximation theorem).\n",
    "- But they will be much more complex, have many more parameters and will be much harder to train.\n",
    "- CNNs are a more efficient architecture for certain kinds of problems, namely image categorization.\n",
    "\n",
    "![Model](./images/image-17.png)\n",
    "\n",
    ">**Parameters of Pooling:**\n",
    "- *Spatial extent (\"kernel size\")*: The number of pixels in the pooling window. Typically set to 2 (actuall 2x2)\n",
    "- *Stride*: The number of pixels to skip for each window. Typically set to 2 (produces no overlap).\n",
    "- Can also use (3, 3). Setting stride < kernel creates overlapping windows, which is less common.\n",
    "\n",
    ">**Code:**\n",
    "- [Part 5 - Max Mean pooling](https://github.com/Sayan-Roy-729/Data-Science/blob/main/Deep%20Learning/Using%20Pytorch/Part%2014%20-%20Convolution/Part%205%20-%20Max%20Mean%20pooling.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e91e5b",
   "metadata": {},
   "source": [
    "## To Pool or To Stride?\n",
    "\n",
    "![ Pool vs Stride](./images/image-18.png)\n",
    "\n",
    "| Pooling | Stride |\n",
    "| :--: | :--: |\n",
    "| Computationally fast | Somewhat slower |\n",
    "| No parameters | Learned parameters |\n",
    "| Kernel spans a smaller area (smaller receptive fields) | Kernel spans a larger area (larger receptive fields) |\n",
    "| Highly stable | Can be unstable in complex architectures |\n",
    "\n",
    ">**Conclusion:** Pooling is historical. No one really knows when to use which. Try both!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6792f924",
   "metadata": {},
   "source": [
    "## Image Transform\n",
    "\n",
    ">**Twpo reasons to transform images:**\n",
    "- Pre-trained CNNs are coded for certain image sizes. You might need to resize your images to work, or convert to grayscale.\n",
    "- Transforming images changes raw pixel values without changing the image information. Transforms are thus a way to increase the total amount of data.\n",
    "\n",
    ">**Code:**\n",
    "- [Part 6 - Image Transformation](https://github.com/Sayan-Roy-729/Data-Science/blob/main/Deep%20Learning/Using%20Pytorch/Part%2014%20-%20Convolution/Part%206%20-%20Image%20Transformation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119c85b0",
   "metadata": {},
   "source": [
    "## Creating & Using Custom Datasets\n",
    "\n",
    "![Data Loader](./images/image-19.png)\n",
    "\n",
    ">**Order of operations when applying transformation:**\n",
    "1. Inport the data\n",
    "2. Create a custom DataSet class\n",
    "3. Define the transformations\n",
    "4. Create a DatasSet with your data and transformation.\n",
    "5. Create a DataLoader (same as usual).\n",
    "\n",
    "*Step 1 and Step 2 will be combined if import a `torchvision` dataset that already allows added transformations*\n",
    "\n",
    ">**Code:**\n",
    "- [Part 7 - Creating and using custom DataSets](https://github.com/Sayan-Roy-729/Data-Science/blob/main/Deep%20Learning/Using%20Pytorch/Part%2014%20-%20Convolution/Part%207%20-%20Creating%20and%20using%20custom%20DataSets.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1bb1d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
