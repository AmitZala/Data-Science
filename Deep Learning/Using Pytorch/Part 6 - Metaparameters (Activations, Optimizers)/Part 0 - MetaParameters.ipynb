{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df341fad",
   "metadata": {},
   "source": [
    "# MetaParameters (Activations & Optimizers)\n",
    "\n",
    "- **Parameters:** Features of the model that are learned by the algorithm (mainly, the weights between modes). You do not set the parameters.\n",
    "\n",
    "- **Metaparameters:** Features of the model that are set by you, not learned automatically by the model.\n",
    "\n",
    "![Metaparameters List](./images/image-1.png)\n",
    "\n",
    "- It is simply impossible to search the entire metaparameters space.\n",
    "- It is difficult to know whether you are using the \"best\" model for your problem.\n",
    "- Fortunately, parametric experiments on some metapameters are feasible.\n",
    "- Ultimately, you must use a combination of experience, intution, previous successes, and empirical exploration.\n",
    "\n",
    "## The `Wine Quality` Dataset:\n",
    "\n",
    "**Code:**\n",
    "- Part 1 - Analysis Wine Dataset\n",
    "- Part 2 - MiniBatch Size in the Wine Dataset\n",
    "\n",
    "## Data Normalization:\n",
    "\n",
    "![Data Normalization](./images/image-2.png)\n",
    "![Data Normalization](./images/image-3.png)\n",
    "\n",
    "**The main points (Data normalization helps ensure that):**\n",
    "- All samples are processed the same.\n",
    "- All data features are treated the same.\n",
    "- Weights remain numerically stable.\n",
    "\n",
    "### Z-Transform\n",
    "\n",
    "$$z_i = \\frac{x_i - \\bar{x}}{\\sigma_x}$$\n",
    "\n",
    "- **Mean center:** Subtract the average from each individual value.\n",
    "- **Variance-normalize:** Divide by the standard deviation.\n",
    "- The units are standard deviations away from the mean of the distribution\n",
    "- Z-transform shifts and stretches, but doesn't change shape.\n",
    "\n",
    "### MinMax Scaling\n",
    "\n",
    "$$\\tilde{x} = \\frac{x - min(x)}{max(x) - min(x)}$$\n",
    "\n",
    "- Scale to range of 0 to 1\n",
    "- Scale to a range of $a$ to $b$. $x^* = a + \\tilde{x}(b-a)$\n",
    "\n",
    "![Normal Distribution](./images/image-4.png)\n",
    "\n",
    "**Code:**\n",
    "- Use previous Regularization iris notebooks by modifying\n",
    "\n",
    "## Batch Normalization\n",
    "**Normalize the input... which input?**\n",
    "- Activation distribution characteristics (mean, variance) can shift as they pass through the layers.\n",
    "- This can lead to covariance shifts, or to vanishing or exploding gradients.\n",
    "- Solution: Normalize inputs to each layer.\n",
    "- Even better: Learn mean/variance parameters for the normalization.\n",
    "\n",
    ">$$y = \\sigma(\\tilde{X}^TW) \\text{ where } \\tilde{X} \\text{ is normalized input}$$\n",
    "\n",
    ">$$\\tilde{X} = \\gamma{X} + \\beta \\text{ where } X \\text{ is Raw input }$$\n",
    "\n",
    "- **Implementation note: BatchNorm goes before the activation function.**\n",
    "- Batch normalization should only be applied during training.\n",
    "- It should be switched off during validation/test, because batch size could differ (e.g., N=1). Instead, the model applies the learned parameters from training.\n",
    "- PyTorch does this for you with `net.eval()`.\n",
    "- Batch normalization applies to the inputs to each layer, not to the minibatches (although it is computed per minibatch)\n",
    "- Normalization is mostly useful for deep networks or datasets with low accuracy.\n",
    "- Acts as a regularizer, because the input distributions are shifted and stabilized.\n",
    "\n",
    "**Code:**\n",
    "- Part 3 - Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7862e9f4",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "**Reminder:** All DL models (no matter how deep) with linear activation functions are simply 1-layer regression models.\n",
    "\n",
    "![Desired qualities in an activation function](./images/image-5.png)\n",
    "\n",
    "![Different Activation Functions](./images/image-6.png)\n",
    "\n",
    "![ReLU variants](./images/image-7.png)\n",
    "\n",
    ">**Comprehensive list of activation functions in neural networks with pros/cons [Click here](https://stats.stackexchange.com/questions/115258/comprehensive-list-of-activation-functions-in-neural-networks-with-pros-cons)**\n",
    "\n",
    ">**How to pick an activation function?**\n",
    "- Start with commonly used functions (ReLU & Sigmoid)\n",
    "- Experiment with other functions if performance is low.\n",
    "- Use only train/dev sets! Don't compare activation functions in the test set.\n",
    "- Keep in mind that some activation functions are developed for academic purposes and may not have been rigorously tested in a variety architectures.\n",
    "\n",
    ">**Code:**\n",
    "- Part 4 - Activation Functions\n",
    "- Part 5 - Activation Functions Comparison\n",
    "- Part 6 - CodeChallenge Compare ReLU Variants\n",
    "- Part 7 - CodeChallenge Predict Sugar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9661fd",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "![MSE & Cross Entropy](./images/image-8.png)\n",
    "\n",
    "![(Binary) Cross Entropy](./images/image-9.png)\n",
    "\n",
    "![Binary vs Multi Class Cross Entropy](./images/image-10.png)\n",
    "\n",
    "![KL Distance Loss Function](./images/image-11.png)\n",
    "\n",
    "> **Output layer architectures:**\n",
    "- `Predict continuous data`:\n",
    "    - Output layer has one unit with linear activation function\n",
    "    - MSE loss function\n",
    "- `Binary classification`:\n",
    "    - Output layer has one unit with sigmoid activation function.\n",
    "    - Cross-entropy (BCE) loss function.\n",
    "- `Multiclass (N-way) classification`:\n",
    "    - Output layer has N units with softmax activation functions.\n",
    "    - Cross-entropy (CCE) loss function\n",
    "    \n",
    "> **Sigmoid vs Softmax**\n",
    "- Why softmax for multiclass classification:\n",
    "    - Softmax for two categories equals sigmoid. $\\sigma = \\frac{e^z}{\\sum e^{z_i}}$\n",
    "    - Using sigmoid for multiclass categorization doesn't create a probability distribution.\n",
    "    - Sigmoid rewards correct responses; softmax also penalizes incorrect responses.\n",
    "    \n",
    "> **Softmax vs log-softmax**\n",
    "- Log-Softmax: Compute softmax then take $log(p)$\n",
    "- Log-softmax increase the sensitivity at small probabilities.\n",
    "- Log-softmax gives a stronger penalty for error.\n",
    "\n",
    "> **Code:**\n",
    "- Part 8 - Loss Functions\n",
    "- Part 9 - Multioutput ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ad6983",
   "metadata": {},
   "source": [
    "## Optimizers (MiniBatch, Momemtum)\n",
    "\n",
    "![Purpose of Optimizers](./images/image-12.png)\n",
    "\n",
    ">**Stochastic Gradient Descent:**\n",
    "$$w ← w - \\eta{dL}$$\n",
    "- Change the weights after each sample. This is great when all samples are similar to each other.\n",
    "- But SGD learning is very sensitive and can lead to volatile changes based on non-representative samples.\n",
    "\n",
    ">**SGD on mini-batches**\n",
    "- Change the weights after N samples. Average losses across the N samples.\n",
    "- Sometimes more robust than SGD, except when samples are similar to each other.\n",
    "\n",
    "![What is momentum](./images/image-13.png)\n",
    "\n",
    "![SGD with momentum](./images/image-14.png)\n",
    "\n",
    "![Why do they call it momentum?](./images/image-15.png)\n",
    "\n",
    ">**Code:**\n",
    "- Part 10 - SGD with Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7523dbbc",
   "metadata": {},
   "source": [
    "## Optimizers (RMSProp, Adam):\n",
    "\n",
    "### RMSprop\n",
    "\n",
    "- RMS = root-mean-square (prop = propagation)\n",
    "- Similar concept as momentum: Bias the weight changes using dampened previous gradients.\n",
    "- RMSprop solution: Instead of biasing the gradient, bias the learning rate according to the magnitude of the gradient.\n",
    "\n",
    "$$rms = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}x_i^2}$$\n",
    "\n",
    "$$std = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2}$$\n",
    "\n",
    "![RMSprop](./images/image-16.png)\n",
    "\n",
    "![RMSprop](./images/image-17.png)\n",
    "\n",
    "### Adam\n",
    "\n",
    "- **Adam (non-DL):** Male name (convenient for alphabetical order)\n",
    "- **Adam (DL):** Adaptive momentum\n",
    "- **A'Dam:** Amsterdam (Dutch city)\n",
    "\n",
    "**Combine momentum and RMSprop.**\n",
    "\n",
    "![Adam Math](./images/image-18.png)\n",
    "\n",
    "**Recommended parameter settings:**\n",
    "- $\\eta = 0.001$\n",
    "- $\\beta_1 = 0.9$\n",
    "- $\\beta_2 = 0.999$\n",
    "- $\\epsilon = 10^{-8}$ (1E-8)\n",
    "\n",
    "**Bias correction factor**\n",
    "\n",
    "$$\\tilde{v} = \\frac{v}{1-\\beta_1^t}$$\n",
    "\n",
    "$$\\tilde{s} = \\frac{s}{1 - \\beta_2^t}$$\n",
    "\n",
    "**where $t$ is the corresponding current training epoch.**\n",
    "\n",
    "**Optimizers galore!**\n",
    "- There are several other optimizers that have been propesed and tested.\n",
    "- Adam is generally considered the current best, so you can use it without losing sleep.\n",
    "- But SGD may work better for smaller-scale models/datasets.\n",
    "- DL is a developing field; Adam may someday be replaced by something better and faster.\n",
    "\n",
    "**Code:**\n",
    "- Part 11 - Optimizers Comparison\n",
    "- Part 12 - CodeChallenge Optimizers & Learning Rate\n",
    "- Part 13 - CodeChallenge Adam with L2 Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba857cf",
   "metadata": {},
   "source": [
    "## Learning Rate Decay\n",
    "\n",
    "- A dynamic learning rate can speed up learning.\n",
    "\n",
    "**Code:**\n",
    "- Part 14 - Learning Rate Decay\n",
    "\n",
    "## How To Pick Right Metaparameters\n",
    "\n",
    "**So, how do you pick metaparameters?**\n",
    "- Most important: Do what others have done (with similar architecure, data, problem, etc)\n",
    "- Use knowledge that you've built up from experience (that's why we test many models and code challenges)\n",
    "- Random search is usually more fruitful than grid-search.\n",
    "- Try to balance laziness and diligence.\n",
    "\n",
    "**Code:**\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "231.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
