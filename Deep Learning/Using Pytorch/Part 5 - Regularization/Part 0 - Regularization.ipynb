{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebea6a21",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "## Concept & Methods:\n",
    "\n",
    "**Why regularize?**\n",
    "- Penalizes \"memorization\" (over-learning examples).\n",
    "- Helps the model generalize to unseen examples.\n",
    "- Changes the representations of learning (either more sparse or more distributed depending on the regularizer)\n",
    "- Can increase or decrease training time.\n",
    "- Can descrease training accuracy but increase generalization.\n",
    "- Works better for large models with multiple hidden layers.\n",
    "- Generally works better with sufficient data.\n",
    "\n",
    "<b>Three families of regularizers in Deep Learning</b>\n",
    "- **Family 1:** Modify the model (dropout)\n",
    "- **Family 2:** Add a cost to the loss function (L1/L2)\n",
    "- **Family 3:** Modify or add data (batch training, data augmentation)\n",
    "\n",
    "<p style=\"display:flex\">\n",
    "    <img src=\"https://raw.githubusercontent.com/Sayan-Roy-729/Data-Science/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/images/image-1.png?token=GHSAT0AAAAAABY4P3FR5AETPZKJ43H3KJVSYZMA7WQ\" width=450 style=\"border-radius:10px\"/>\n",
    "    <img src=\"https://raw.githubusercontent.com/Sayan-Roy-729/Data-Science/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/images/image-2.png?token=GHSAT0AAAAAABY4P3FRSBE5GP74OMM2L6Q6YZMBAKQ\" width=500 style=\"border-radius:10px\"/>\n",
    "</p>\n",
    "\n",
    "<p style=\"display:flex\">\n",
    "    <img src=\"https://raw.githubusercontent.com/Sayan-Roy-729/Data-Science/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/images/image-3.png?token=GHSAT0AAAAAABY4P3FRXW4A36U42GDF6BTWYZMBAYA\" width=450 style=\"border-radius:10px\"/>\n",
    "    <img src=\"https://raw.githubusercontent.com/Sayan-Roy-729/Data-Science/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/images/image-4.png?token=GHSAT0AAAAAABY4P3FQR3EGF6G7KVGQTBOYYZMBBBQ\" width=450 style=\"border-radius:10px\"/>\n",
    "</p>\n",
    "\n",
    "**How to think about regularization:**\n",
    "- Adds a cost to the complexity of the solution\n",
    "- Forces the solution to be smooth\n",
    "- Prevents the model from learning item-specific details.\n",
    "\n",
    "**Which regularization method to use?**\n",
    "<p>\n",
    "    <img src=\"https://raw.githubusercontent.com/Sayan-Roy-729/Data-Science/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/images/image-5.png?token=GHSAT0AAAAAABY4P3FRWMQC7H4FKM56X7RWYZMBBLA\" width=500 style=\"border-radius:10px\"/>\n",
    "</p>\n",
    "\n",
    "## `train()` & `eval()` method:\n",
    "\n",
    "**Training vs evaluation mode:**\n",
    "- Gradients are computed only during backpropagation, not during evaluation.\n",
    "- Some regularization methods are applied only during training, not during evaluation.\n",
    "- Ergo: We need a way to deactivate gradient computations and regularization while evaluating model performance.\n",
    "\n",
    "<p style=\"display:flex\">\n",
    "    <img src=\"https://raw.githubusercontent.com/Sayan-Roy-729/Data-Science/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/images/image-6.png?token=GHSAT0AAAAAABY4P3FRUAVB7XMU2UPOUEJAYZMBCAQ\" width=450 style=\"border-radius:10px\"/>\n",
    "    <img src=\"https://raw.githubusercontent.com/Sayan-Roy-729/Data-Science/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/images/image-7.png?token=GHSAT0AAAAAABY4P3FQKRRGFFUMP2RHQMDMYZMBCLA\" width=500 style=\"border-radius:10px\"/>\n",
    "</p>\n",
    "\n",
    "## DropOut Regularization:\n",
    "\n",
    "<p style=\"display:flex\">\n",
    "    <img src=\"https://raw.githubusercontent.com/Sayan-Roy-729/Data-Science/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/images/image-8.png?token=GHSAT0AAAAAABY4P3FQ6DSURO5FRQYHVS3YYZMBCUA\" width=450 style=\"border-radius:10px\"/>\n",
    "    <img src=\"https://raw.githubusercontent.com/Sayan-Roy-729/Data-Science/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/images/image-9.png?token=GHSAT0AAAAAABY4P3FR6P64AJRYZDZWYF4SYZMBC4A\" width=450 style=\"border-radius:10px\"/>\n",
    "</p>\n",
    "\n",
    "<p style=\"display:flex\">\n",
    "    <img src=\"https://raw.githubusercontent.com/Sayan-Roy-729/Data-Science/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/images/image-10.png?token=GHSAT0AAAAAABY4P3FQQMKRPQ4QCNNNYTXOYZMBDGA\" width=450 style=\"border-radius:10px\"/>\n",
    "    <img src=\"https://raw.githubusercontent.com/Sayan-Roy-729/Data-Science/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/images/image-11.png?token=GHSAT0AAAAAABY4P3FQMDOW64P2UFNZMTGYYZMBDPA\" width=450 style=\"border-radius:10px\"/>\n",
    "</p>\n",
    "\n",
    "**How it works?**\n",
    "- Dropout reduces the overall activation (fewer elements in the weighted sum)\n",
    "- Solutions\n",
    "    - Scale up the weights during training\n",
    "    - Scale down the weights during testing.\n",
    "\n",
    "**Effects:**\n",
    "- Prevents a single node from learning too much\n",
    "- Forces the model to have distributed representations.\n",
    "- Makes the model less reliant or individual nodes and thus more stable.\n",
    "\n",
    "**Other observations:**\n",
    "- Generally requires more training (though each epoch computes faster).\n",
    "- Can desrease training accuracy but increase generalization.\n",
    "- Usually works better on deep than sallow networks.\n",
    "- Debate about applying it to convolution layers.\n",
    "- Works better with sufficient data, unnecessary with \"enough\" data.\n",
    "\n",
    "**Code**\n",
    "- [Part 1 - DropOut In Pytorch](https://github.com/Sayan-Roy-729/Data-Science/blob/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/Part%201%20-%20DropOut%20In%20Pytorch.ipynb)\n",
    "- [Part 2 - DropOut Regularization by Building Model](https://github.com/Sayan-Roy-729/Data-Science/blob/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/Part%202%20-%20DropOut%20Regularization%20by%20Building%20Model.ipynb)\n",
    "- [Part 3 - Dropout on Iris Dataset](https://github.com/Sayan-Roy-729/Data-Science/blob/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/Part%203%20-%20Dropout%20on%20Iris%20Dataset.ipynb)\n",
    "\n",
    "## L1 & L2 Regularization\n",
    "\n",
    "<p style=\"display:flex\">\n",
    "    <img src=\"https://raw.githubusercontent.com/Sayan-Roy-729/Data-Science/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/images/image-12.png?token=GHSAT0AAAAAABY4P3FQIDNMVOHCWWYJEKBOYZMBD6A\" style=\"border-radius:10px\"/>\n",
    "</p>\n",
    "\n",
    "<p style=\"display:flex\">\n",
    "    <img src=\"https://raw.githubusercontent.com/Sayan-Roy-729/Data-Science/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/images/image-13.png?token=GHSAT0AAAAAABY4P3FQYPLDMN4BYGDCIFJMYZMBEGA\" width=450 style=\"border-radius:10px\"/>\n",
    "    <img src=\"https://raw.githubusercontent.com/Sayan-Roy-729/Data-Science/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/images/image-14.png?token=GHSAT0AAAAAABY4P3FRZJ6QIPPQ7PS47XWQYZMBEOQ\" width=450 style=\"border-radius:10px\"/>\n",
    "</p>\n",
    "\n",
    "<p style=\"display:flex\">\n",
    "    <img src=\"https://raw.githubusercontent.com/Sayan-Roy-729/Data-Science/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/images/image-15.png?token=GHSAT0AAAAAABY4P3FRTV6D2XQUAVXLCSKIYZMBEWQ\" width=450 style=\"border-radius:10px\"/>\n",
    "</p>\n",
    "\n",
    "**What else to regularize for?**\n",
    "- L1 + L1 (\"elastic net\" regression)\n",
    "- Norm of weight matrix\n",
    "- Sample-specific (e.g., positive bias on cancer diagnosis)\n",
    "\n",
    "**Why does regularization reduce overfitting?**\n",
    "- Discourages complex and sample-specific representations.\n",
    "- Prevents overfitting to training examples.\n",
    "- Large weights lead to instability (very different outputs for similar inputs).\n",
    "\n",
    "**When to use L1/L2 regularization?**\n",
    "- In large, complex models with lots of weights (high risk of overfitting)\n",
    "- Use L1 when trying to understand the important encoding features (more common in regression than DL)\n",
    "- When training accuracy is much higher than validation accuracy.\n",
    "\n",
    "**Code:**\n",
    "- [Part 4 - L2 Regularization](https://github.com/Sayan-Roy-729/Data-Science/blob/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/Part%204%20-%20L2%20Regularization.ipynb)\n",
    "- [Part 5 - L1 Regularization](https://github.com/Sayan-Roy-729/Data-Science/blob/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/Part%205%20-%20L1%20Regularization.ipynb)\n",
    "\n",
    "## Training in Mini-Batches\n",
    "<p style=\"display:flex\">\n",
    "    <img src=\"https://raw.githubusercontent.com/Sayan-Roy-729/Data-Science/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/images/image-16.png?token=GHSAT0AAAAAABY4P3FRLR7NRBOHV3GVUFJUYZMBE7A\" width=450 style=\"border-radius:10px\"/>\n",
    "</p>\n",
    "\n",
    "**How and why to train with mini-batches?**\n",
    "- Batch size is often power of 2 (e.g., $2^4 = 16$), between 2 and 512.\n",
    "- Training in batches can decrease computation time because of vectorization (matrix multiplication instead of for-loops).\n",
    "- But batching can increase computation time for large batches and large data samples (e.g., images)\n",
    "- Batching is a form of regularization: It smooths learning by averaging the loss over many samples, and thereby reduces overfitting.\n",
    "- If samples are highly similar, minibatch=1 can give faster training.\n",
    "\n",
    "**Mini-Batch Analogy**\n",
    "- Imagine you take an exam with 100 questions.\n",
    "- SGD: Teacher gives you detailed feedback on each answer. This is good for learning but very time consuming.\n",
    "- One batch: Teacher gives you a final exam score with no feedback. Grading is fast, but it's difficult to learn from your mistakes.\n",
    "- Min-Batch: Teacher gives you a separate grade and feedback on average performance of blocks of 10 questions. This balances apeed and learning ability.\n",
    "\n",
    "**Code:**\n",
    "- [Part 6 - MiniBatch](https://github.com/Sayan-Roy-729/Data-Science/blob/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/Part%206%20-%20MiniBatch.ipynb)\n",
    "- [Part 7 - Importance of Equal Batch Sizes](https://github.com/Sayan-Roy-729/Data-Science/blob/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/Part%207%20-%20Importance%20of%20Equal%20Batch%20Sizes.ipynb)\n",
    "- [Part 8 - CodeChallenge Effects of Mini-Batch Size](https://github.com/Sayan-Roy-729/Data-Science/blob/main/Deep%20Learning/Using%20Pytorch/Part%205%20-%20Regularization/Part%208%20-%20CodeChallenge%20Effects%20of%20Mini-Batch%20Size.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d483482a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "207.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
