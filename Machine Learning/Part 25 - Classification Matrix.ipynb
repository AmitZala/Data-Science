{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce4d3320",
   "metadata": {},
   "source": [
    "# Accuracy\n",
    "\n",
    "$$accuracy = \\frac{\\text{no. of correct predictions}}{\\text{total no. of predictions}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f2c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# y_test is truth and y_pred is the prediction\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5a90b9",
   "metadata": {},
   "source": [
    "**How much accuracy is good?**\n",
    "\n",
    "It depends on the basis of the problem.\n",
    "\n",
    "**The problem with accuracy matrix:**\n",
    "\n",
    "- It does not tell the type of the error. Let's say a model is used to preduct that the student will get the placement or not and accuracy is 90%. So the 10% is not accurate. I dont know that the model is predicted that the student will get the placement but in reality he will not get. And same as the second type where model predicted that the student will not get the placement but in reality he will get. You will never know the types of the inaccuracy types.\n",
    "\n",
    "# Confusion Metrix\n",
    "\n",
    "![Type1 & Type2 Error](https://cdn.inblog.in/user/uploads/edZRgLLVcZjPQogeKsmOk4lZq2tF5z.jpg)\n",
    "\n",
    "**According to Scikit learn, the truth values are to the left side and predicted values are to the upper side**\n",
    "\n",
    "$$accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "**Confusion matrix for multi-class classification problem**\n",
    "\n",
    "| Actual ↓ Predicted → | 0 | 1 | 2 |\n",
    "| :--: | :--: | :--: | :--: |\n",
    "| 0 | 7 | 0 | 5 |\n",
    "| 1 | 2 | 21 | 6 |\n",
    "| 2 | 9 | 1 | 13 |\n",
    "\n",
    "**When accuracy is misleading?**\n",
    "- Imbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d4b17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c31ef4a",
   "metadata": {},
   "source": [
    "# Precision Metrics\n",
    "\n",
    "![Spam Classifier](./images/image-8.png)\n",
    "\n",
    "From these 2 models, we can see that False Positive of left side model is greater than False Positive of right side model. And where as False Negative of left model is less than false negative of right side model.\n",
    "\n",
    "$$FP_{left} > FP_{right} | FN_{left} < FN_{right}$$\n",
    "\n",
    "So, right side model is more accurate. **What proportion of predicted Positives is truly positive?**\n",
    "\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "So, precision of left side model = $\\frac{100}{100 + 30}$ and precision of right side model = $\\frac{100}{100 + 10}$. So $Precision_{left} < Precision_{right}$\n",
    "\n",
    "# Recall\n",
    "\n",
    "![Recall](./images/image-9.png)\n",
    "\n",
    "According to the problem in this case, left side model is better than right side model.\n",
    "\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "So, recoall of left side model = $\\frac{1000}{1200}$ and recall of right side model = $\\frac{1000}{1500}$. So $R_{left} > R_{right}$\n",
    "\n",
    "# F1 Score\n",
    "\n",
    "**If you can't decide that is type-I error more dangerous or type-II more dangerous, this is solved by F1 Score. It is the combination of Precesion and Recall.**\n",
    "\n",
    "$$F1 = \\frac{2*Precision*Recall}{Precision + Recall}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7552578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd41d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision score\n",
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d03ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall score\n",
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56b44ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 score\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e0a1c5",
   "metadata": {},
   "source": [
    "# Multi-Class Precision & Recall\n",
    "\n",
    "| | Dog | Cat | Rabbit | Total |\n",
    "| :--: | :--: | :--: | :--: | :--: |\n",
    "| **Dog** | 25 | 5 | 10 | 40 |\n",
    "| **Cat** | 0 | 30 | 4 | 34 |\n",
    "| **Rabbit** | 4 | 10 | 20 | 34 |\n",
    "| **Total** | 29 | 45 | 34 |  |\n",
    "\n",
    "$Precision_{dog} = \\frac{25}{29} = 0.86$\n",
    "\n",
    "$Precision_{cat} = \\frac{30}{45} = 0.66$\n",
    "\n",
    "$Precision_{rabbit} = \\frac{20}{34} = 0.58$\n",
    "\n",
    "$\\text{Macro Precision} = \\frac{0.86 + 0.66 + 0.58}{3} = 0.7$\n",
    "\n",
    "$\\text{Weighted Precision} = \\frac{40}{108}*0.86 + \\frac{34}{108} * 0.64 + \\frac{34}{108}*0.58 = 0.71$\n",
    "\n",
    "-------------------------------------------------------------------------\n",
    "$Recall_{dog} = \\frac{25}{40} = 0.62$\n",
    "\n",
    "$Recall_{cat} = \\frac{30}{34} = 0.88$\n",
    "\n",
    "$Recall_{rabbit} = \\frac{20}{34} = 0.58$\n",
    "\n",
    "$\\text{Macro Recall} = \\frac{0.62 + 0.88 + 0.58}{3} = 0.69$\n",
    "\n",
    "$\\text{Weighted Precision} = \\frac{40}{108} * 0.62 + \\frac{34}{108} * 0.88 + \\frac{34}{108} * 0.58 = 0.68$\n",
    "\n",
    "--------------------------------------------------------------------------\n",
    "$F1_{dog} = \\frac{2P_DR_D}{P_D + R_D} = \\frac{2*0.86*0.62}{0.86 + 0.62} = 0.72$\n",
    "\n",
    "$F1_{cat} = \\frac{2P_CR_C}{P_C + R_C} = \\frac{2*0.66*0.88}{0.66 + 0.88} = 0.75$\n",
    "\n",
    "$F1_{Rabbit} = \\frac{2P_RR_R}{P_R + R_R} = \\frac{2*0.58*0.58}{0.58 + 0.58} = 0.58$\n",
    "\n",
    "$\\text{Macro F1} = \\frac{0.72 + 0.75 + 0.58}{3} = 0.68$\n",
    "\n",
    "$\\text{Weighted F1} = \\frac{40}{108}*0.72 + \\frac{34}{108}*0.75 + \\frac{34}{108}*0.58 = 0.69$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a242756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision_score(y_test, y_pred, average = None)\n",
    "recall_score(y_test, y_pred, average = None)\n",
    "f1_score(y_test, y_pred, average = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7dd80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred, average = \"macro\")\n",
    "recall_score(y_test, y_pred, average = \"macro\")\n",
    "f1_score(y_test, y_pred, average = \"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f893098",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred, average = \"weighted\")\n",
    "recall_score(y_test, y_pred, average = \"weighted\")\n",
    "f1_score(y_test, y_pred, average = \"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51def726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# all calculation together at a time for precision, recall, f1 score, macro, weighted and accuracy\n",
    "classification_report(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
